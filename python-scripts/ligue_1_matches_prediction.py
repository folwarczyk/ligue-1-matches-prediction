# -*- coding: utf-8 -*-
"""ligue-1-matches-prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hcOR2R4XNTtyfd_KNZ9LCvGOfLoDe3qH
"""

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

"""**Loading data:** At the beginning of the code, the required libraries are imported, and then a CSV file containing data from French league matches is loaded."""

import pandas as pd

df = pd.read_csv('ligue-1-matches.csv', index_col=0)

df.head()

df.columns

"""#Data preprocessing:

Removal of redundant columns: Only necessary columns are selected to be used for analysis and prediction of results.
"""

columns_to_keep = ['venue', 'opponent', 'xg', 'xga', 'poss', 'attendance', 'sh', 'sot', 
                   'formation', 'fk', 'pk', 'pkatt', 'season', 'date', 'team', 'result']
df = df[columns_to_keep]

"""Quick overview of selected columns: 
1. **venue**: The venue of the match may affect the result. Teams often play better at home.
2. **opponent**: The strength of the opponent affects the probability of winning.
3. **xg and xga**: These are the "expected goals" stats for the team and its opponent. They can provide information on how well a team has performed in the past.
4. **poss**: Possession of the ball can be a significant factor in the outcome of a match.
5. **attendance**: Match attendance can affect the atmosphere of the match and the final result.
6. **sh and sot**: Shots and shots on goal can be a good indicator of a team's offensive capabilities.
7. **formation**: The formation a team plays in can affect its effectiveness.
8. **fk, pk and pkatt**: Free kicks, penalty kicks and penalty kick attempts can affect the outcome of a match.
9. **season and date**: Time-related trends, such as a team's form in different seasons or at different points in a season, can affect the outcome of a match.

"""

df.head()

df.columns

df.isna().sum()

df.dtypes

"""Supplementing missing data:

You can either delete these rows or fill in the missing values. Deleting the rows is not a good solution because you can lose a lot of important data. Instead, we can fill in the missing values with the mean frequency or zero.
"""

df['attendance'] = df['attendance'].fillna(df['attendance'].mean()) 
# or df['attendance'].fillna(0)

df.isna().sum()

"""Encoding categorical columns:

'*venue*', '*opponent*', '*formation*', '*team*' and '*resutl*' columns are categorical and must be encoded. We can use the LabelEncoder from the sklearn library for this encoding.
"""

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

df['venue'] = le.fit_transform(df['venue'])
df['opponent'] = le.fit_transform(df['opponent'])
df['formation'] = le.fit_transform(df['formation'])
df['team'] = le.fit_transform(df['team'])
df['result'] = le.fit_transform(df['result'])

"""Creating new features:

creating new columns based on the average values of "xg" and "xga" for each team. The average "xg" and "xga" for the team can give us information about the team's overall performance in the past, which can be useful for our model. 


"""

# Calculate mean 'xg' and 'xga' for each team
mean_xg = df.groupby('team')['xg'].mean()
mean_xga = df.groupby('team')['xga'].mean()

# Create new features
df['mean_team_xg'] = df['team'].map(mean_xg)
df['mean_team_xga'] = df['team'].map(mean_xga)

# Drop the original 'xg' and 'xga' columns
#model learned better with these columns
#df = df.drop(columns=['xg', 'xga'])

df.head()

df.columns

"""Change the date format to a Pandas date:"""

df['date'] = pd.to_datetime(df['date'])

df.head()

"""In the first step of this project, the Random Forest model was used as a starting point for predicting match results.



---

Data up to August 5, 2022 were used as training data, and data after that date (the last 2022/2022 season) as test data.
"""

train = df[df["date"] < '2022-08-05']
test = df[df["date"] >= '2022-08-05']

"""Then it was decided what the goal would be (i.e. what we want to predict). We want to predict the outcome of the match (that is, the "result" column)."""

y_train = train["result"]
X_train = train.drop("result", axis=1)

y_test = test["result"]
X_test = test.drop("result", axis=1)

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=50, min_samples_split=10, random_state=1)
rf.fit(X_train, y_train)

"""You can convert a date to numerical characteristics such as year, month, day of the month, day of the week, etc. Depending on the context, different aspects of the date may be important. In the context of predicting the results of matches, the day of the week, the month and maybe even the year can be important."""

#Convert 'date' column to datetime type
X_train['date'] = pd.to_datetime(X_train['date'])
X_test['date'] = pd.to_datetime(X_test['date'])

#Extract features from the date
X_train['year'] = X_train['date'].dt.year
X_train['month'] = X_train['date'].dt.month
X_train['day'] = X_train['date'].dt.day
X_train['dayofweek'] = X_train['date'].dt.dayofweek

X_test['year'] = X_test['date'].dt.year
X_test['month'] = X_test['date'].dt.month
X_test['day'] = X_test['date'].dt.day
X_test['dayofweek'] = X_test['date'].dt.dayofweek

#Drop the original 'date' column
X_train = X_train.drop('date', axis=1)
X_test = X_test.drop('date', axis=1)

"""Random Forest model is created and trained on the training data."""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, min_samples_split=10, random_state=1)
rf.fit(X_train, y_train)

preds = rf.predict(X_test)

"""The accuracy of the model is calculated."""

from sklearn.metrics import accuracy_score

acc = accuracy_score(y_test, preds)
print("Accuracy: ", acc)

"""Using other metrics to evaluate our RandomForest model:

**Precision**: the ratio of true positives to the sum of true positives and false positives. Precision is a measure that tells us how well the model identifies only significant cases.

**Recall**: the ratio of true positives to the sum of true positives and false negatives. Recall tells us how well the model identifies all relevant cases.

**F1 Score**: the harmonic mean of precision and recall. For the F1 Score to be high, both precision and recall must be high.

**Confusion Matrix**: a table that describes the performance of a classification model on a set of data for which the truth is known. This allows you to easily understand what errors the model is making.
"""

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

y_pred = rf.predict(X_test)

print('Precision:', precision_score(y_test, y_pred, average='macro'))
print('Recall:', recall_score(y_test, y_pred, average='macro'))
print('F1 Score:', f1_score(y_test, y_pred, average='macro'))
print('Confusion Matrix:\n', confusion_matrix(y_test, y_pred))

"""Overall accuracy is 0.61, which means that the model correctly predicts about 61% of the cases. For the problem of predicting the results of football matches, this is quite a good result, because it is a task with high uncertainty.

The average precision of the model is 0.56. This means that when the model predicts a team to win, it is correct 56% of the time.

The average Recall of the model is 0.55. This means that out of all the matches that a given team actually won, the model correctly predicted 55% of them. 

The average F1 value is 0.54, which is the harmonic mean of precision and sensitivity. F1 is useful when the costs of false positives and false negatives are uncertain, and it is important to balance precision and sensitivity.

This matrix shows what errors the model makes. From your results, the model tends to predict second class, which may suggest that the model is somewhat biased.


*Overall, the results suggest that the model has some ability to predict the outcome of French league matches, but also can be improved. May be considered using other modeling techniques, adding more features that can help the model better understand the data, or fine-tuning the parameters of the model.*

---

We now turn to the use of the XGBoost model. XGBoost is an implementation of the Gradient Boosting algorithm which is very effective in many classification problems. Here's how we can apply XGBoost to this problem:
"""

from xgboost import XGBClassifier
from sklearn.metrics import classification_report

#Create an XGBoost model
xgb = XGBClassifier(random_state=1)

#Train the model
xgb.fit(X_train, y_train)

#Use the model to make predictions on test data
xgb_preds = xgb.predict(X_test)

#Print the metrics
print('Accuracy: ', accuracy_score(y_test, xgb_preds))
print('Precision: ', precision_score(y_test, xgb_preds, average='macro'))
print('Recall: ', recall_score(y_test, xgb_preds, average='macro'))
print('F1 Score: ', f1_score(y_test, xgb_preds, average='macro'))
print('Confusion Matrix:\n', confusion_matrix(y_test, xgb_preds))

"""The accuracy of the XGBoost model is 0.61, which is slightly higher than the accuracy of the RandomForest model (0.60). This suggests that the XGBoost model is slightly better at predicting the results of French league matches.

The average precision of the XGBoost model is 0.57, which is slightly higher than that of the RandomForest model (0.56). This means that the XGBoost model is slightly more accurate in predicting match winners.

The average sensitivity of the XGBoost model is 0.57, which is also slightly higher compared to the RandomForest model (0.55). This means that the XGBoost model is better at identifying real winners.

The average F1 value for the XGBoost model is 0.57, which is slightly higher compared to the RandomForest model (0.54). This means that the XGBoost model better balances precision and sensitivity.

Looking at the confusion matrix, it can be seen that the number of false predictions has decreased for each class compared to the RandomForest model. This suggests that the XGBoost model is a bit more reliable.

In conclusion, the XGBoost model seems to improve the results compared to the RandomForest model in the context of predicting the results of French league matches based on the provided metrics. However, it is important to note that these differences are minor and both models have similar results. Still, XGBoost seems to be a better choice based on the data provided.

---
**Creating own model:**
"""

df.head()

#Importing the necessary libraries
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras import regularizers

from sklearn.preprocessing import StandardScaler
from keras.utils import to_categorical

#Data scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

#Converting labels to one-hot form
y_train_categorical = to_categorical(y_train)
y_test_categorical = to_categorical(y_test)

#Define the model
model = Sequential()
input_dim = X_train.shape[1]  #number of features

model.add(Dense(64, input_dim=input_dim, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
model.add(Dropout(0.2))  # Dodaj warstwę Dropout
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))  # Dodaj warstwę Dropout
model.add(Dense(16, activation='relu'))
model.add(Dense(3, activation='softmax'))

#Model compile
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

#Model training
history = model.fit(X_train, y_train_categorical, validation_data=(X_test, y_test_categorical), epochs=100, batch_size=10)

"""The sequential model from Keras was used, which means that the layers were added one after the other. The first layer is the Dense layer (full connection layer) with 64 neurons, ReLU activation function and L2 regularization. Added a Dropout layer with a rate of 0.2, which means 20% of neurons are randomly turned off during training to prevent overfitting. The addition of Dense and Dropout layers continued, and the last layer is a Dense layer with 3 neurons (one per class) and a softmax activation function, which is typically used in multi-class classification."""

#Rate the model
loss, accuracy = model.evaluate(X_test, y_test_categorical, verbose=0)

print('Test loss:', loss)
print('Test accuracy:', accuracy)

from numpy import argmax

predictions = model.predict(X_test)

predicted_labels = argmax(predictions, axis=1)

from sklearn.metrics import classification_report, confusion_matrix

#Generate classification report
print(classification_report(y_test, predicted_labels))

#Generate confusion matrix
print(confusion_matrix(y_test, predicted_labels))

"""Create the table with the real and predicted result for one team (in this example Paris Saint-Germain):"""

team_encoder = LabelEncoder()
team_encoder.fit(df['team'])
team_decoder = {code: team for code, team in enumerate(team_encoder.classes_)}

opponent_encoder = LabelEncoder()
opponent_encoder.fit(df['opponent'])
opponent_decoder = {code: opponent for code, opponent in enumerate(opponent_encoder.classes_)}

#Create a new DataFrame with match information
results_df = pd.DataFrame(columns=['team_A', 'team_B', 'home/away', 'date', 'result', 'predicted_result'])

team_code = 21
team_name = team_decoder[team_code]

import numpy as np

for i, row in test.iterrows():
    if row['team'] == team_code or row['opponent'] == team_code:
        team_A = team_decoder[row['team']]
        team_B = opponent_decoder[row['opponent']]
        home_away = 'Home' if row['venue'] == 'home' else 'Away'
        date = row['date']
        result = row['result']
        predicted_probs = model.predict(np.expand_dims(X_test[i], axis=0))[0]
        predicted_result = np.argmax(predicted_probs)

        results_df = results_df.append({'team_A': team_A, 'team_B': team_B, 'home/away': home_away,
                                        'date': date, 'result': result, 'predicted_result': predicted_result},
                                       ignore_index=True)

print(results_df)

results_df

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Convert y_test to numpy array
y_test_array = y_test.to_numpy()

print('Accuracy:', accuracy_score(y_test_array, predicted_labels))
print('Precision:', precision_score(y_test_array, predicted_labels, average='macro'))
print('Recall:', recall_score(y_test_array, predicted_labels, average='macro'))
print('F1 Score:', f1_score(y_test_array, predicted_labels, average='macro'))

"""#CONCLUSIONS

Analyzing the results obtained from the neural network model for predicting the results of football matches, we see that the accuracy of the model is 58%. This means that the model correctly predicted the outcome of approximately 58% of the test matches. This is quite a good result, given the complex nature of predicting the outcome of football matches, where many variables can affect the final result.

Looking more closely at the metrics for each class, we see that Precision, Recall, and F1-scores are relatively low for class 0. This suggests that the model is having a hard time predicting that particular class. It is possible that the model needs more data for this class, or that there are some features that are unique to this class that are difficult for the model to capture.

For classes 1 and 2, the metric values are higher, suggesting that the model is better at predicting these classes.

The confusion matrix further illustrates where the model is making mistakes. For example, the model often confuses classes 0 and 1, suggesting that the two classes may be difficult to distinguish based on available features.

Compared to the **Random Forest** and **XGBoost** models, the neural network has comparable accuracy, suggesting that neither model is significantly better than the other for this particular problem.



---
However, it is important to emphasize that predicting the outcome of football matches is a very complex task. Football is an unpredictable sport and the outcome of matches can be affected by many factors that are difficult to account for in the model, such as team strategy, player injuries, player form, and even factors such as weather conditions and mental strain. In fact, even the most accurate prediction models may not be able to anticipate the surprises that are a common feature of football.

---

When analyzing the results of machine learning models in the context of predicting football match results, we noticed that even the most efficient model (XGBoost) achieved an accuracy of around 61%. The model based on the neural network achieved an accuracy of about 58%. These results highlight the difficulty of the problem - football is an unpredictable sport where the outcome of a match can change at any minute.

While these models may provide some information, they do not always reflect the full dynamics of a match. It is important to understand that modeling such a complex field as football requires taking into account many variables that can affect the result. This understanding is one of the key lessons learned from this project.

During the construction of the models, I experimented with various parameters and features, studying their impact on the quality of predictions. After many tests and research, I selected a set of features that provided the most promising results, achieving accuracy in the range of 58% to 60%. This process underscores the importance of the feature selection and model tuning process in machine learning, while demonstrating that even after careful selection of features and parameters, the model's accuracy in predicting football outcomes is limited by the unpredictability of the sport itself.

When predicting match outcomes, an alternative approach might be to look for additional data that may be useful, such as player stats, injury data, and even weather conditions. We can also explore other modeling techniques. 



"""